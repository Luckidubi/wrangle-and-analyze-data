{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report for Udacity Project Wrangle and Analyze Data\n",
    "\n",
    "\n",
    "## WeRateDogs Twitter Archive Analysis\n",
    " \n",
    "This project involves wrangling data from WeRateDogs Twitter archive. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. The datasets were retrieved from three different sources and file formats. Below are the different sources:\n",
    "\n",
    "### Enhanced Twitter Archive\n",
    "This dataset comes in a CSV file sent to Udacity by WeRateDogs to be used in this project. It contains tweet data such as tweet ID, timestamp, text, source, name, etc. The tweets found on this dataset were created on or before August 1st, 2017.\n",
    "\n",
    "### Twitter API\n",
    "This dataset contains the retweet_count and favorite_count and was obtained by querying Twitter API using the Python Tweepy library. The tweet ID from the Twitter archive dataset was used to retrieve similar tweet data in JSON format from Twitter API. \n",
    "\n",
    "### Image Predictions File\n",
    "The Image Prediction file comes in TSV file format. It contains tweet ID, image url and top three image predictions of dogs from the Twitter archive dataset using a neural network that can classify breeds of dogs.\n",
    "\n",
    "There are 3 processes involved in wrangling the datasets which include:\n",
    "- Gathering\n",
    "- Assessing\n",
    "- Cleaning\n",
    "\n",
    "### Gathering\n",
    "- **Enhanced Twitter Archive**: This dataset was downloaded manually through this [link](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv). After downloading the dataset it was read into a pandas dataframe using the read_csv function.\n",
    "```Python\n",
    "# Loading the twitter dataset in a dataframe\n",
    "df_tweet = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "```\n",
    "- **Image Prediction file**: The image prediction file which is in tsv format was downloaded programmatically from the internet with this [url](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) into a new folder using a Python's library called requests. \n",
    "```Python\n",
    "response = requests.get(url)\n",
    "with open(os.path.join(folder_name, url.split('/')[-1]), mode='wb') as file:\n",
    "    file.write(response.content)\n",
    "```\n",
    "After downloading and saving the file in a folder, it was then read into a pandas dataframe\n",
    "```Python\n",
    "# Loading the downloaded image file into a dataframe\n",
    "df_image_prediction = pd.read_csv('my_project/image-predictions.tsv', sep='\\t')\n",
    "```\n",
    "- **Twitter API**: The twitter api is a closed source api, twitter requires an offical application in order to grant elevated access to their api. After getting the elevated access I was given an api_key, api_secret_key, access_token and access_secret. With the api keys I was able to connect and make call on the twitter api\n",
    "```Python\n",
    "# Connecting to the Twitter api\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "```\n",
    "After connecting, downloading, and saving the tweets in a tweet_json.txt file. The dataset was then loaded into a dataframe\n",
    "```Python\n",
    "# Loading the tweet dataset into a dataframe\n",
    "df_tweet_api = pd.DataFrame(df_tweet_list, columns=['tweet_id', 'retweet_count', 'favorite_count'])\n",
    "```\n",
    "\n",
    "### Accessing\n",
    "After assessing the three datasets visually and programmatically. I found 8 quality issues and 3 tidiness issues:\n",
    "\n",
    "#### Quality issues\n",
    "- Four quality issues were found in the Twitter archive dataset\n",
    "- Four quality issues were also found in the Image prediction dataset\n",
    "\n",
    "#### Tidiness Issues\n",
    "- Two tidiness issues in the Twitter archive dataset\n",
    "- One tidiness issue involves combining the three datasets to form one master dataset.\n",
    "\n",
    "\n",
    "### Cleaning\n",
    "Before the cleaning process, copies of the three datasets were made. The cleaning was performed on the copies not the original. The following steps were taken to clean the dataset:\n",
    "- Removing rows with null values\n",
    "- Removing rows that are confirmed to be retweeted and replied tweets\n",
    "- Dropping rows with duplicates\n",
    "- Dropping irrelevant columns\n",
    "- Changing erroneous datatype to correct datatype\n",
    "- Joining two or more variables into one \n",
    "- Merging datasets together to form a complete observational unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing\n",
    "\n",
    "After the whole wrangling process, the cleaned datasets was merged and stored in a master dataset file called twitter_archive_master.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
